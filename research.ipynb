{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-02-06T14:34:52.879980Z",
     "start_time": "2025-02-06T14:34:51.881528Z"
    }
   },
   "source": [
    "!pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in ./.venv/lib/python3.11/site-packages (1.6.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "84c567bceec3502e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T14:35:38.329393Z",
     "start_time": "2025-02-06T14:35:03.141001Z"
    }
   },
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import ast\n",
    "import certifi\n",
    "import ssl\n",
    "\n",
    "# =======================\n",
    "# Configuration Constants\n",
    "# =======================\n",
    "OLLAMA_API_KEY = \"ollamaKey\" # Replace with your Ollama API key\n",
    "SERPAPI_API_KEY = \"SERPAPIKEY\" # Replace with your SERPAPI API key\n",
    "JINA_API_KEY = \"JINAKEY\" # Replace with your JINA API key\n",
    "\n",
    "# Endpoints\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "SERPAPI_URL = \"https://serpapi.com/search\"\n",
    "JINA_BASE_URL = \"https://r.jina.ai/\"\n",
    "\n",
    "# Default LLM model (can be changed if desired)\n",
    "DEFAULT_MODEL = \"llama3.2\"\n",
    "\n",
    "# Output cleanup helper functions\n",
    "# Add these helper functions\n",
    "def clean_query_string(query):\n",
    "    #Ensure proper string formatting\n",
    "    return query.strip().replace('\"', '').replace(\"'\", \"\").split(\":\")[-1].strip()\n",
    "\n",
    "def validate_python_list(text):\n",
    "    #Convert malformed list strings to valid Python lists\n",
    "    text = text.replace('\\n', '').replace('“', '\"').replace('”', '\"')\n",
    "    text = re.sub(r',\\s*]', ']', text)  # Fix trailing commas\n",
    "    text = re.sub(r',\\s*,', ',', text)  # Fix double commas\n",
    "    return text\n",
    "\n",
    "# ============================\n",
    "# Asynchronous Helper Functions\n",
    "# ============================\n",
    "\n",
    "async def call_ollama_async(session, messages, model=DEFAULT_MODEL):\n",
    "    \"\"\"\n",
    "    Asynchronously call the Ollama API with the provided messages.\n",
    "    Returns the content of the assistant’s reply.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OLLAMA_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"num_ctx\": 6144,\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        async with session.post(OLLAMA_URL, headers=headers, json=payload) as resp:\n",
    "            if resp.status == 200:\n",
    "                result = await resp.json()\n",
    "                # Correctly parse Ollama's chat response\n",
    "                if 'message' in result:\n",
    "                    return result['message']['content']\n",
    "                else:\n",
    "                    print(\"Unexpected Ollama response structure:\", result)\n",
    "                    return None\n",
    "            else:\n",
    "                text = await resp.text()\n",
    "                print(f\"Ollama API error: {resp.status} - {text}\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(\"Error calling Ollama:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "async def generate_search_queries_async(session, user_query):\n",
    "    \"\"\"\n",
    "    Ask the LLM to produce up to four precise search queries (in Python list format)\n",
    "    based on the user’s query.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are an expert research assistant. Given the user's query, generate up to four distinct, \"\n",
    "        \"precise search queries that would help gather comprehensive information on the topic. \"\n",
    "        \"Return ONLY a Python list of strings formatted like this: ['query1', 'query2', 'query3'].\"\n",
    "        \"Do not include any explanations, code formatting, or additional text.\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful and precise research assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\n{prompt}\"}\n",
    "    ]\n",
    "    response = await call_ollama_async(session, messages)\n",
    "    if response:\n",
    "        try:\n",
    "            # Clean response before parsing\n",
    "            cleaned_response = response.split(\"```\")[0].strip()  # Remove markdown code blocks\n",
    "            cleaned_response = cleaned_response.split(\"Here are\")[0].strip()  # Remove introductory text\n",
    "\n",
    "            # Find first valid list pattern using regex\n",
    "            list_match = re.search(r'\\[.*?\\]', cleaned_response, re.DOTALL)\n",
    "            if not list_match:\n",
    "                return []\n",
    "\n",
    "            # Use ast.literal_eval for safer parsing\n",
    "            search_queries = ast.literal_eval(list_match.group(0))\n",
    "\n",
    "            if isinstance(search_queries, list):\n",
    "                return search_queries\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing search queries:\", e, \"\\nResponse:\", response)\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "\n",
    "async def perform_search_async(session, query):\n",
    "    \"\"\"\n",
    "    Asynchronously perform a Google search using SERPAPI for the given query.\n",
    "    Returns a list of result URLs.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"api_key\": SERPAPI_API_KEY,\n",
    "        \"engine\": \"google\"\n",
    "    }\n",
    "    try:\n",
    "        async with session.get(SERPAPI_URL, params=params) as resp:\n",
    "            if resp.status == 200:\n",
    "                results = await resp.json()\n",
    "                if \"organic_results\" in results:\n",
    "                    links = [item.get(\"link\") for item in results[\"organic_results\"] if \"link\" in item]\n",
    "                    return links\n",
    "                else:\n",
    "                    print(\"No organic results in SERPAPI response.\")\n",
    "                    return []\n",
    "            else:\n",
    "                text = await resp.text()\n",
    "                print(f\"SERPAPI error: {resp.status} - {text}\")\n",
    "                return []\n",
    "    except Exception as e:\n",
    "        print(\"Error performing SERPAPI search:\", e)\n",
    "        return []\n",
    "\n",
    "\n",
    "async def fetch_webpage_text_async(session, url):\n",
    "    \"\"\"\n",
    "    Asynchronously retrieve the text content of a webpage using Jina.\n",
    "    The URL is appended to the Jina endpoint.\n",
    "    \"\"\"\n",
    "    full_url = f\"{JINA_BASE_URL}{url}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {JINA_API_KEY}\"\n",
    "    }\n",
    "    try:\n",
    "        async with session.get(full_url, headers=headers) as resp:\n",
    "            if resp.status == 200:\n",
    "                return await resp.text()\n",
    "            else:\n",
    "                text = await resp.text()\n",
    "                print(f\"Jina fetch error for {url}: {resp.status} - {text}\")\n",
    "                return \"\"\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching webpage text with Jina:\", e)\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "async def is_page_useful_async(session, user_query, page_text):\n",
    "    \"\"\"\n",
    "    Ask the LLM if the provided webpage content is useful for answering the user's query.\n",
    "    The LLM must reply with exactly \"Yes\" or \"No\".\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a critical research evaluator. Given the user's query and the content of a webpage, \"\n",
    "        \"determine if the webpage contains information relevant and useful for addressing the query. \"\n",
    "        \"Respond with exactly one word: 'Yes' if the page is useful, or 'No' if it is not. Do not include any extra text.\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a strict and concise evaluator of research relevance.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\nWebpage Content (first 20000 characters):\\n{page_text[:20000]}\\n\\n{prompt}\"}\n",
    "    ]\n",
    "    response = await call_ollama_async(session, messages)\n",
    "    if response:\n",
    "        answer = response.strip()\n",
    "        if answer in [\"Yes\", \"No\"]:\n",
    "            return answer\n",
    "        else:\n",
    "            # Fallback: try to extract Yes/No from the response.\n",
    "            if \"Yes\" in answer:\n",
    "                return \"Yes\"\n",
    "            elif \"No\" in answer:\n",
    "                return \"No\"\n",
    "    return \"No\"\n",
    "\n",
    "\n",
    "async def extract_relevant_context_async(session, user_query, search_query, page_text):\n",
    "    \"\"\"\n",
    "    Given the original query, the search query used, and the page content,\n",
    "    have the LLM extract all information relevant for answering the query.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are an expert information extractor. Given the user's query, the search query that led to this page, \"\n",
    "        \"and the webpage content, extract all pieces of information that are relevant to answering the user's query. \"\n",
    "        \"Return only the relevant context as plain text without commentary.\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in extracting and summarizing relevant information.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nSearch Query: {search_query}\\n\\nWebpage Content (first 20000 characters):\\n{page_text[:20000]}\\n\\n{prompt}\"}\n",
    "    ]\n",
    "    response = await call_ollama_async(session, messages)\n",
    "    if response:\n",
    "        return response.strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "async def get_new_search_queries_async(session, user_query, previous_search_queries, all_contexts):\n",
    "    \"\"\"\n",
    "    Based on the original query, the previously used search queries, and all the extracted contexts,\n",
    "    ask the LLM whether additional search queries are needed. If yes, return a Python list of up to four queries;\n",
    "    if the LLM thinks research is complete, it should return \"\".\n",
    "    \"\"\"\n",
    "    context_combined = \"\\n\".join(all_contexts)\n",
    "    prompt = (\n",
    "        \"You are an analytical research assistant. Based on the original query, the search queries performed so far, \"\n",
    "        \"and the extracted contexts from webpages, determine if further research is needed. \"\n",
    "        \"If further research is needed, provide up to four new search queries as a Python list (for example, \"\n",
    "        \"['new query1', 'new query2']). If you believe no further research is needed, respond with exactly .\"\n",
    "        \"\\nOutput only a Python list or the token  without any additional text.\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a systematic research planner.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nPrevious Search Queries: {previous_search_queries}\\n\\nExtracted Relevant Contexts:\\n{context_combined}\\n\\n{prompt}\"}\n",
    "    ]\n",
    "    response = await call_ollama_async(session, messages)\n",
    "    if response:\n",
    "        try:\n",
    "            # Clean response in stages\n",
    "            cleaned = response.strip()\n",
    "\n",
    "            # Remove markdown code blocks\n",
    "            cleaned = cleaned.split(\"```\")[0].strip()\n",
    "\n",
    "            # Remove numbered labels and colons\n",
    "            cleaned = re.sub(r'New Query\\d+:\\s*\"?', '', cleaned)\n",
    "\n",
    "            # Remove internal quotes\n",
    "            cleaned = cleaned.replace('\"', '').replace(\"'\", \"\")\n",
    "\n",
    "            # Find first valid list pattern\n",
    "            list_match = re.search(r'\\[.*?\\]', cleaned, re.DOTALL)\n",
    "            if not list_match:\n",
    "                return []\n",
    "\n",
    "            # Safely evaluate\n",
    "            import ast\n",
    "            new_queries = ast.literal_eval(list_match.group(0))\n",
    "\n",
    "            # Final validation\n",
    "            if isinstance(new_queries, list) and all(isinstance(x, str) for x in new_queries):\n",
    "                return new_queries\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"Parse error: {e}\\nCleaned response: {cleaned}\")\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "\n",
    "async def generate_final_report_async(session, user_query, all_contexts):\n",
    "    \"\"\"\n",
    "    Generate the final comprehensive report using all gathered contexts.\n",
    "    \"\"\"\n",
    "    context_combined = \"\\n\".join(all_contexts)\n",
    "    prompt = (\n",
    "        \"You are an expert researcher and report writer. Based on the gathered contexts below and the original query, \"\n",
    "        \"write a comprehensive, well-structured, and detailed report that addresses the query thoroughly. \"\n",
    "        \"Include all relevant insights and conclusions without extraneous commentary.\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a skilled report writer.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\nGathered Relevant Contexts:\\n{context_combined}\\n\\n{prompt}\"}\n",
    "    ]\n",
    "    report = await call_ollama_async(session, messages)\n",
    "    return report\n",
    "\n",
    "\n",
    "async def process_link(session, link, user_query, search_query):\n",
    "    \"\"\"\n",
    "    Process a single link: fetch its content, judge its usefulness, and if useful, extract the relevant context.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching content from: {link}\")\n",
    "    page_text = await fetch_webpage_text_async(session, link)\n",
    "    if not page_text:\n",
    "        return None\n",
    "    usefulness = await is_page_useful_async(session, user_query, page_text)\n",
    "    print(f\"Page usefulness for {link}: {usefulness}\")\n",
    "    if usefulness == \"Yes\":\n",
    "        context = await extract_relevant_context_async(session, user_query, search_query, page_text)\n",
    "        if context:\n",
    "            print(f\"Extracted context from {link} (first 200 chars): {context[:200]}\")\n",
    "            return context\n",
    "    return None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main Asynchronous Routine\n",
    "# =========================\n",
    "\n",
    "async def async_main():\n",
    "    user_query = input(\"Enter your research query/topic: \").strip()\n",
    "    iter_limit_input = input(\"Enter maximum number of iterations (default 10): \").strip()\n",
    "    iteration_limit = int(iter_limit_input) if iter_limit_input.isdigit() else 10\n",
    "\n",
    "    aggregated_contexts = []    # All useful contexts from every iteration\n",
    "    all_search_queries = []     # Every search query used across iterations\n",
    "    iteration = 0\n",
    "\n",
    "    ssl_context = ssl.create_default_context(cafile=certifi.where())\n",
    "    connector = aiohttp.TCPConnector(ssl=ssl_context)\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        # ----- INITIAL SEARCH QUERIES -----\n",
    "        new_search_queries = await generate_search_queries_async(session, user_query)\n",
    "        if not new_search_queries:\n",
    "            print(\"No search queries were generated by the LLM. Exiting.\")\n",
    "            return\n",
    "        all_search_queries.extend(new_search_queries)\n",
    "\n",
    "        # ----- ITERATIVE RESEARCH LOOP -----\n",
    "        while iteration < iteration_limit:\n",
    "            print(f\"\\n=== Iteration {iteration + 1} ===\")\n",
    "            iteration_contexts = []\n",
    "\n",
    "            # For each search query, perform SERPAPI searches concurrently.\n",
    "            search_tasks = [perform_search_async(session, query) for query in new_search_queries]\n",
    "            search_results = await asyncio.gather(*search_tasks)\n",
    "\n",
    "            # Aggregate all unique links from all search queries of this iteration.\n",
    "            # Map each unique link to the search query that produced it.\n",
    "            unique_links = {}\n",
    "            for idx, links in enumerate(search_results):\n",
    "                query = new_search_queries[idx]\n",
    "                for link in links:\n",
    "                    if link not in unique_links:\n",
    "                        unique_links[link] = query\n",
    "\n",
    "            print(f\"Aggregated {len(unique_links)} unique links from this iteration.\")\n",
    "\n",
    "            # Process each link concurrently: fetch, judge, and extract context.\n",
    "            link_tasks = [\n",
    "                process_link(session, link, user_query, unique_links[link])\n",
    "                for link in unique_links\n",
    "            ]\n",
    "            link_results = await asyncio.gather(*link_tasks)\n",
    "\n",
    "            # Collect non-None contexts.\n",
    "            for res in link_results:\n",
    "                if res:\n",
    "                    iteration_contexts.append(res)\n",
    "\n",
    "            if iteration_contexts:\n",
    "                aggregated_contexts.extend(iteration_contexts)\n",
    "            else:\n",
    "                print(\"No useful contexts were found in this iteration.\")\n",
    "\n",
    "            # ----- ASK THE LLM IF MORE SEARCHES ARE NEEDED -----\n",
    "            new_search_queries = await get_new_search_queries_async(session, user_query, all_search_queries, aggregated_contexts)\n",
    "            if new_search_queries == \"\":\n",
    "                print(\"LLM indicated that no further research is needed.\")\n",
    "                break\n",
    "            elif new_search_queries:\n",
    "                print(\"LLM provided new search queries:\", new_search_queries)\n",
    "                all_search_queries.extend(new_search_queries)\n",
    "            else:\n",
    "                print(\"LLM did not provide any new search queries. Ending the loop.\")\n",
    "                break\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        # ----- FINAL REPORT -----\n",
    "        print(\"\\nGenerating final report...\")\n",
    "        final_report = await generate_final_report_async(session, user_query, aggregated_contexts)\n",
    "        print(\"\\n==== FINAL REPORT ====\\n\")\n",
    "        print(final_report)\n",
    "\n",
    "\n",
    "def main():\n",
    "    asyncio.run(async_main())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 1 ===\n",
      "SERPAPI error: 401 - {\n",
      "  \"error\": \"Invalid API key. Your API key should be here: https://serpapi.com/manage-api-key\"\n",
      "}\n",
      "SERPAPI error: 401 - {\n",
      "  \"error\": \"Invalid API key. Your API key should be here: https://serpapi.com/manage-api-key\"\n",
      "}\n",
      "SERPAPI error: 401 - {\n",
      "  \"error\": \"Invalid API key. Your API key should be here: https://serpapi.com/manage-api-key\"\n",
      "}\n",
      "SERPAPI error: 401 - {\n",
      "  \"error\": \"Invalid API key. Your API key should be here: https://serpapi.com/manage-api-key\"\n",
      "}\n",
      "Aggregated 0 unique links from this iteration.\n",
      "No useful contexts were found in this iteration.\n",
      "Parse error: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\n",
      "Cleaned response: [bread manufacturing process overview, industrial yeast usage in commercial bread production, automatic baking machine design considerations, large scale oven temperature control]\n",
      "LLM did not provide any new search queries. Ending the loop.\n",
      "\n",
      "Generating final report...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 391\u001B[0m\n\u001B[1;32m    387\u001B[0m     asyncio\u001B[38;5;241m.\u001B[39mrun(async_main())\n\u001B[1;32m    390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 391\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 387\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mmain\u001B[39m():\n\u001B[0;32m--> 387\u001B[0m     \u001B[43masyncio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43masync_main\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Developer/OpenResearch/.venv/lib/python3.11/site-packages/nest_asyncio.py:30\u001B[0m, in \u001B[0;36m_patch_asyncio.<locals>.run\u001B[0;34m(main, debug)\u001B[0m\n\u001B[1;32m     28\u001B[0m task \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mensure_future(main)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m task\u001B[38;5;241m.\u001B[39mdone():\n",
      "File \u001B[0;32m~/Developer/OpenResearch/.venv/lib/python3.11/site-packages/nest_asyncio.py:92\u001B[0m, in \u001B[0;36m_patch_loop.<locals>.run_until_complete\u001B[0;34m(self, future)\u001B[0m\n\u001B[1;32m     90\u001B[0m     f\u001B[38;5;241m.\u001B[39m_log_destroy_pending \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m f\u001B[38;5;241m.\u001B[39mdone():\n\u001B[0;32m---> 92\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stopping:\n\u001B[1;32m     94\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/Developer/OpenResearch/.venv/lib/python3.11/site-packages/nest_asyncio.py:115\u001B[0m, in \u001B[0;36m_patch_loop.<locals>._run_once\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    108\u001B[0m     heappop(scheduled)\n\u001B[1;32m    110\u001B[0m timeout \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ready \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stopping\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mmax\u001B[39m(\n\u001B[1;32m    113\u001B[0m         scheduled[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m_when \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtime(), \u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m86400\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m scheduled\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 115\u001B[0m event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_events(event_list)\n\u001B[1;32m    118\u001B[0m end_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clock_resolution\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:566\u001B[0m, in \u001B[0;36mKqueueSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    564\u001B[0m ready \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    565\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 566\u001B[0m     kev_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_selector\u001B[38;5;241m.\u001B[39mcontrol(\u001B[38;5;28;01mNone\u001B[39;00m, max_ev, timeout)\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ready\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
